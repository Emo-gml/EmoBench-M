# EmoBench-M: Benchmarking Emotional Intelligence for Multimodal Large Language Models


## ğŸ“¢ Update: 2025.07.08

ğŸ‰ğŸ‰ğŸ‰ The **EmoBench-M** benchmark for multimodal emotion understanding is now available on **Google Drive**!

ğŸ“ [Click to access the Google Drive link](https://drive.google.com/file/d/16MAChQR2ASjL_gk24bGVnBxlV3ukoVoh/view) <!-- Replace with actual link -->

Feel free to explore, evaluate, and build upon it! ğŸ¤—


ğŸ“š Read the paper: [arXiv PDF](https://arxiv.org/pdf/2502.04424) | [arXiv Page](https://arxiv.org/abs/2502.04424)

ğŸŒ [**Project Homepage**](https://emo-gml.github.io/)  
ğŸ“Š [**Dataset**](https://drive.google.com/file/d/16MAChQR2ASjL_gk24bGVnBxlV3ukoVoh/view)

---

## Introduction
Can Multimodal Large Language Models (MLLMs) understand human emotions in dynamic, multimodal settings? To address this question, we introduce <b>EmoBench-M</b>, a comprehensive benchmark grounded in psychological theories of Emotional Intelligence (EI), designed to evaluate the EI capabilities of MLLMs across video, audio, and text. <b>EmoBench-M</b> spans 13 diverse scenarios across three key dimensions of EI: Foundational Emotion Recognition, Conversational Emotion Understanding, and Socially Complex Emotion Analysis. It includes over 5000 carefully curated samples and both classification and generation tasks, covering a wide range of real-world affective contexts. Through extensive evaluations of state-of-the-art MLLMsâ€”including open-source models like Qwen2.5-VL and InternVL2.5, and proprietary models such as Gemini 2.0 Flashâ€”we find that (i) current MLLMs significantly lag behind human performance, especially in conversational and socially complex tasks; (ii) model size alone does not guarantee better emotional reasoning; and (iii) nuanced social emotions and intent understanding remain particularly challenging. We hope EmoBench-M provides a solid foundation for future research toward emotionally intelligent AI systems.


![Alt text](images/intro_1.png)

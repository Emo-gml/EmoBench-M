# EmoBench-M

<p align="center">
  <img src="https://raw.githubusercontent.com/Emo-gml/Emo-gml.github.io/master/emo.jpg" alt="project logo" width="200px" />
</p>

<p align="center">
    <a href="https://emo-gml.github.io/"><img src="https://img.shields.io/badge/%F0%9F%8F%86-website-8A2BE2"></a>
    <a href="https://arxiv.org/abs/2502.04424"><img src="https://img.shields.io/badge/arXiv-2502.04424-b31b1b.svg"></a>
    <a href="https://drive.google.com/file/d/16MAChQR2ASjL_gk24bGVnBxlV3ukoVoh/view"><img src="https://img.shields.io/badge/Dataset-Open-green.svg" alt="Dataset Open"/></a>
    <a href="https://opensource.org/licenses/Apache-2.0"><img src="https://img.shields.io/badge/License-Apache%202.0-blue.svg" alt="License: Apache 2.0"/> </a>

</p>

<p align="center">
    <a href="#-about">üå∏ About</a> ‚Ä¢
    <a href="#-news">üì∞ News</a> ‚Ä¢
    <a href="#-quick-start">üî• Quick Start</a> ‚Ä¢
    <a href="#-citation">üìú Citation</a>
</p>

## üå∏ About
This repository contains the official evaluation code and data for the paper "**EmoBench-M: Benchmarking Emotional Intelligence for Multimodal Large Language Models**". See more details in our [paper](https://arxiv.org/pdf/2502.04424).

> Can Multimodal Large Language Models (MLLMs) understand human emotions in dynamic, multimodal settings? To address this question, we introduce <b>EmoBench-M</b>, a comprehensive benchmark grounded in psychological theories of Emotional Intelligence (EI), designed to evaluate the EI capabilities of MLLMs across video, audio, and text. <b>EmoBench-M</b> spans 13 diverse scenarios across three key dimensions of EI: Foundational Emotion Recognition, Conversational Emotion Understanding, and Socially Complex Emotion Analysis. It includes over 5000 carefully curated samples and both classification and generation tasks, covering a wide range of real-world affective contexts. Through extensive evaluations of state-of-the-art MLLMs‚Äîincluding open-source models like Qwen2.5-VL and InternVL2.5, and proprietary models such as Gemini 2.0 Flash‚Äîwe find that (i) current MLLMs significantly lag behind human performance, especially in conversational and socially complex tasks; (ii) model size alone does not guarantee better emotional reasoning; and (iii) nuanced social emotions and intent understanding remain particularly challenging. We hope EmoBench-M provides a solid foundation for future research toward emotionally intelligent AI systems.

![Alt text](images/intro_1.png)

## üì∞ News
- **[2025-07-08]** We open-sourced the code and dataset for EmoBench-M on GitHub!
- **[2025-02-06]** Paper submitted to arXiv: https://arxiv.org/abs/2502.04424„ÄÇ
- **[2025-02-05]** Created the official project website: https://emo-gml.github.io/.


## üî• Quick Start
## üì• Download Data

To use this benchmark, **please first download the original video files and corresponding annotation `.json` files** from the link below:

üîó **[Download Videos & JSONs](https://drive.google.com/file/d/16MAChQR2ASjL_gk24bGVnBxlV3ukoVoh/view)**  

Each JSON file contains conversation-style prompts and labels aligned with the corresponding video clips. The structure looks like:

```json
[
  {
    "id": "0",
    "video": "videos/ch-simsv2s/aqgy4_0004/00023.mp4",
    "conversations": [
      {
        "from": "human",
        "value": "<video>\nThe person in video says: ... Determine the emotion conveyed..."
      },
      {
        "from": "gpt",
        "value": "negative"
      }
    ]
  }
]
```
## üß™ Evaluation Usage

### Install Dependencies

```bash
pip install -r requirements.txt
```

### 1. Classification

```bash
python eval.py classification --json results.json --output classification.json
```

Format:
```json
[
  {"video": "sample1.mp4", "expected_value": "positive", "predicted_value": "positive"},
  {"video": "sample2.mp4", "expected_value": "neutral", "predicted_value": "negative"}
]
```

---

### 2. Joint Emotion + Intent

```bash
python eval.py joint --json emotions.json --output joint.json
```

Format:
```json
[
  {
    "modal_path": "sample1.mp4",
    "expected_emotion": "happy",
    "predicted_emotion": "happy",
    "expected_intent": "encouraging",
    "predicted_intent": "encouraging"
  }
]
```

---

### 3. Generation

```bash
python eval.py generation --json gen.json --output generation.json
```

Format:
```json
[
  {"video": "sample1.mp4", "prediction": "I am very happy", "reference": "I feel happy"}
]
```

---

### 4. Run All in One

```bash
python eval.py all \
  --classification-json results.json \
  --joint-json emotions.json \
  --generation-json gen.json \
  --output-dir results/
```

Output:
- `results/classification.json`
- `results/joint.json`
- `results/generation.json`

---

## üìÅ Provided Files

- `eval.py` ‚Äî evaluation script (supports all modes)
- `requirements.txt` ‚Äî required Python packages
- `results.json` ‚Äî classification sample
- `emotions.json` ‚Äî joint sample
- `gen.json` ‚Äî generation sample


## üìú Citation
```bibtex
@article{hu2025emobench,
  title={EmoBench-M: Benchmarking Emotional Intelligence for Multimodal Large Language Models},
  author={Hu, He and Zhou, Yucheng and You, Lianzhong and Xu, Hongbo and Wang, Qianning and Lian, Zheng and Yu, Fei Richard and Ma, Fei and Cui, Laizhong},
  journal={arXiv preprint arXiv:2502.04424},
  year={2025}
  }
```

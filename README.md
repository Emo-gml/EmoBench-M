# EmoBench-M

<p align="center">
  <img src="https://raw.githubusercontent.com/Emo-gml/Emo-gml.github.io/master/emo.jpg" alt="project logo" width="200px" />
</p>

<p align="center">
    <a href="https://emo-gml.github.io/"><img src="https://img.shields.io/badge/%F0%9F%8F%86-website-8A2BE2"></a>
    <a href="https://arxiv.org/abs/2502.04424"><img src="https://img.shields.io/badge/arXiv-2502.04424-b31b1b.svg"></a>
    <a href="https://drive.google.com/file/d/16MAChQR2ASjL_gk24bGVnBxlV3ukoVoh/view"><img src="https://img.shields.io/badge/Dataset-Open-green.svg" alt="Dataset Open"/></a>
    <a href="https://opensource.org/licenses/Apache-2.0"><img src="https://img.shields.io/badge/License-Apache%202.0-blue.svg" alt="License: Apache 2.0"/> </a>

</p>

<p align="center">
    <a href="#-about">ðŸŒ¸ About</a> â€¢
    <a href="#-news">ðŸ“° News</a> â€¢
    <a href="#-quick-start">ðŸ”¥ Quick Start</a> â€¢
    <a href="#-citation">ðŸ“œ Citation</a>
</p>

## ðŸŒ¸ About
This repository contains the official evaluation code and data for the paper "**EmoBench-M: Benchmarking Emotional Intelligence for Multimodal Large Language Models**". See more details in our [paper](https://arxiv.org/pdf/2502.04424).

> Can Multimodal Large Language Models (MLLMs) understand human emotions in dynamic, multimodal settings? To address this question, we introduce <b>EmoBench-M</b>, a comprehensive benchmark grounded in psychological theories of Emotional Intelligence (EI), designed to evaluate the EI capabilities of MLLMs across video, audio, and text. <b>EmoBench-M</b> spans 13 diverse scenarios across three key dimensions of EI: Foundational Emotion Recognition, Conversational Emotion Understanding, and Socially Complex Emotion Analysis. It includes over 5000 carefully curated samples and both classification and generation tasks, covering a wide range of real-world affective contexts. Through extensive evaluations of state-of-the-art MLLMsâ€”including open-source models like Qwen2.5-VL and InternVL2.5, and proprietary models such as Gemini 2.0 Flashâ€”we find that (i) current MLLMs significantly lag behind human performance, especially in conversational and socially complex tasks; (ii) model size alone does not guarantee better emotional reasoning; and (iii) nuanced social emotions and intent understanding remain particularly challenging. We hope EmoBench-M provides a solid foundation for future research toward emotionally intelligent AI systems.

![Alt text](images/intro_1.png)

## ðŸ“° News
- **[2025-07-08]** We open-sourced the code and dataset for EmoBench-M on GitHub!
- **[2025-02-06]** Paper submitted to arXiv: https://arxiv.org/abs/2502.04424ã€‚
- **[2025-02-05]** Created the official project website: https://emo-gml.github.io/.


## ðŸ”¥ Quick Start
## ðŸ“¥ Download Data

To use this benchmark, **please first download the original video files and corresponding annotation `.json` files** from the link below:

ðŸ”— **[Download Videos & JSONs](YOUR_LINK_HERE)**  
*(Replace `YOUR_LINK_HERE` with your actual data hosting link)*

Each JSON file contains conversation-style prompts and labels aligned with the corresponding video clips. The structure looks like:

```json
[
  {
    "id": "0",
    "video": "videos/ch-simsv2s/aqgy4_0004/00023.mp4",
    "conversations": [
      {
        "from": "human",
        "value": "<video>\nThe person in video says: ... Determine the emotion conveyed..."
      },
      {
        "from": "gpt",
        "value": "negative"
      }
    ]
  }
]
```


## ðŸ“œ Citation
```bibtex
@article{hu2025emobench,
  title={EmoBench-M: Benchmarking Emotional Intelligence for Multimodal Large Language Models},
  author={Hu, He and Zhou, Yucheng and You, Lianzhong and Xu, Hongbo and Wang, Qianning and Lian, Zheng and Yu, Fei Richard and Ma, Fei and Cui, Laizhong},
  journal={arXiv preprint arXiv:2502.04424},
  year={2025}
  }
```

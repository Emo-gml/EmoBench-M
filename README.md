# EmoBench-M: Benchmarking Emotional Intelligence for Multimodal Large Language Models

üöÄ **New!** Our paper **"EmoBench-M: Benchmarking Emotional Intelligence for Multimodal Large Language Models"** üéâ  
This repository contains the official evaluation code and data for our work.


üìö Read the paper: [arXiv PDF](https://arxiv.org/pdf/2502.04424) | [arXiv Page](https://arxiv.org/abs/2502.04424)

üåê [**Project Homepage**](https://emo-gml.github.io/)  
üìä [**Dataset**](https://drive.google.com/file/d/16MAChQR2ASjL_gk24bGVnBxlV3ukoVoh/view)

---

## Introduction
Can Multimodal Large Language Models (MLLMs) understand human emotions in dynamic, multimodal settings? To address this question, we introduce <b>EmoBench-M</b>, a comprehensive benchmark grounded in psychological theories of Emotional Intelligence (EI), designed to evaluate the EI capabilities of MLLMs across video, audio, and text. <b>EmoBench-M</b> spans 13 diverse scenarios across three key dimensions of EI: Foundational Emotion Recognition, Conversational Emotion Understanding, and Socially Complex Emotion Analysis. It includes over 5000 carefully curated samples and both classification and generation tasks, covering a wide range of real-world affective contexts. Through extensive evaluations of state-of-the-art MLLMs‚Äîincluding open-source models like Qwen2.5-VL and InternVL2.5, and proprietary models such as Gemini 2.0 Flash‚Äîwe find that (i) current MLLMs significantly lag behind human performance, especially in conversational and socially complex tasks; (ii) model size alone does not guarantee better emotional reasoning; and (iii) nuanced social emotions and intent understanding remain particularly challenging. We hope EmoBench-M provides a solid foundation for future research toward emotionally intelligent AI systems.


![Alt text](images/intro_1.png)

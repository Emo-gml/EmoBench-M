# EmoBench-M: Benchmarking Emotional Intelligence for Multimodal Large Language Models


## ğŸ“¢ Update: 2025.07.08

ğŸ‰ğŸ‰ğŸ‰ The **EmoBench-M** benchmark for multimodal emotion understanding is now available on **Google Drive**!

ğŸ“ [Click to access the Google Drive link](https://drive.google.com/file/d/16MAChQR2ASjL_gk24bGVnBxlV3ukoVoh/view) <!-- Replace with actual link -->

Feel free to explore, evaluate, and build upon it! ğŸ¤—


ğŸ“š Read the paper: [arXiv PDF](https://arxiv.org/pdf/2502.04424) | [arXiv Page](https://arxiv.org/abs/2502.04424)

ğŸŒ [**Project Homepage**](https://emo-gml.github.io/)  
ğŸ“Š [**Dataset**](https://drive.google.com/file/d/16MAChQR2ASjL_gk24bGVnBxlV3ukoVoh/view)

---

## Introduction
Can Multimodal Large Language Models (MLLMs) understand human emotions in dynamic, multimodal settings? To address this question, we introduce <b>EmoBench-M</b>, a comprehensive benchmark grounded in psychological theories of Emotional Intelligence (EI), designed to evaluate the EI capabilities of MLLMs across video, audio, and text. <b>EmoBench-M</b> spans 13 diverse scenarios across three key dimensions of EI: Foundational Emotion Recognition, Conversational Emotion Understanding, and Socially Complex Emotion Analysis. It includes over 5000 carefully curated samples and both classification and generation tasks, covering a wide range of real-world affective contexts. Through extensive evaluations of state-of-the-art MLLMsâ€”including open-source models like Qwen2.5-VL and InternVL2.5, and proprietary models such as Gemini 2.0 Flashâ€”we find that (i) current MLLMs significantly lag behind human performance, especially in conversational and socially complex tasks; (ii) model size alone does not guarantee better emotional reasoning; and (iii) nuanced social emotions and intent understanding remain particularly challenging. We hope EmoBench-M provides a solid foundation for future research toward emotionally intelligent AI systems.


![Alt text](images/intro_1.png)




# EmoBench-M

<p align="center">
  <img src="https://raw.githubusercontent.com/Emo-gml/Emo-gml.github.io/master/emo.jpg" alt="project logo" width="200px" />
</p>

<p align="center">
    <a href="https://emo-gml.github.io/"><img src="https://img.shields.io/badge/%F0%9F%8F%86-website-8A2BE2"></a>
    <a href="https://arxiv.org/abs/2502.04424"><img src="https://img.shields.io/badge/arXiv-2502.04424-b31b1b.svg"></a>
    <a href="https://drive.google.com/file/d/16MAChQR2ASjL_gk24bGVnBxlV3ukoVoh/view"><img src="https://img.shields.io/badge/Dataset-Open-green.svg" alt="Dataset Open"/></a>
    <a href="https://opensource.org/licenses/Apache-2.0"><img src="https://img.shields.io/badge/License-Apache%202.0-blue.svg" alt="License: Apache 2.0"/> </a>

</p>

<p align="center">
    <a href="#-about">ğŸŒ¸ About</a> â€¢
    <a href="#-news">ğŸ“° News</a> â€¢
    <a href="#-quick-start">ğŸ”¥ Quick Start</a> â€¢
    <a href="#-citation">ğŸ“œ Citation</a>
</p>

## ğŸŒ¸ About
This repository contains the official evaluation code and data for the paper "**EmoBench-M: Benchmarking Emotional Intelligence for Multimodal Large Language Models**". See more details in our [paper](https://arxiv.org/pdf/2502.04424).

> Can Multimodal Large Language Models (MLLMs) understand human emotions in dynamic, multimodal settings? To address this question, we introduce <b>EmoBench-M</b>, a comprehensive benchmark grounded in psychological theories of Emotional Intelligence (EI), designed to evaluate the EI capabilities of MLLMs across video, audio, and text. <b>EmoBench-M</b> spans 13 diverse scenarios across three key dimensions of EI: Foundational Emotion Recognition, Conversational Emotion Understanding, and Socially Complex Emotion Analysis. It includes over 5000 carefully curated samples and both classification and generation tasks, covering a wide range of real-world affective contexts. Through extensive evaluations of state-of-the-art MLLMsâ€”including open-source models like Qwen2.5-VL and InternVL2.5, and proprietary models such as Gemini 2.0 Flashâ€”we find that (i) current MLLMs significantly lag behind human performance, especially in conversational and socially complex tasks; (ii) model size alone does not guarantee better emotional reasoning; and (iii) nuanced social emotions and intent understanding remain particularly challenging. We hope EmoBench-M provides a solid foundation for future research toward emotionally intelligent AI systems.

![Alt text](images/intro_1.png)

## ğŸ“° News
- **[2025-07-08]** We open-sourced the code and dataset for EmoBench-M on GitHub!
- **[2025-02-06]** Paper submitted to arXiv: https://arxiv.org/abs/2502.04424ã€‚
- **[2025-02-05]** Created the official project website: https://emo-gml.github.io/.


## ğŸ”¥ Quick Start



## ğŸ“œ Citation
```bibtex
@article{hu2025emobench,
  title={EmoBench-M: Benchmarking Emotional Intelligence for Multimodal Large Language Models},
  author={Hu, He and Zhou, Yucheng and You, Lianzhong and Xu, Hongbo and Wang, Qianning and Lian, Zheng and Yu, Fei Richard and Ma, Fei and Cui, Laizhong},
  journal={arXiv preprint arXiv:2502.04424},
  year={2025}
  }
```

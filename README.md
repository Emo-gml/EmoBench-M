# EmoBench-M: Benchmarking Emotional Intelligence for Multimodal Large Language Models

ğŸš€ **New!** Our paper **"EmoBench-M: Benchmarking Emotional Intelligence for Multimodal Large Language Models"** ğŸ‰  
This repository contains the official evaluation code and data for our work.


ğŸ“š Read the paper: [arXiv PDF](https://arxiv.org/pdf/2503.14939v1) | [arXiv Page](https://arxiv.org/abs/2503.14939v1)

ğŸŒ [**Project Homepage**](https://emo-gml.github.io/)  
ğŸ“Š [**Dataset**](https://drive.google.com/file/d/16MAChQR2ASjL_gk24bGVnBxlV3ukoVoh/view)

---

## Introduction
<b>EmoBench-M<b> is the first benchmark to integrate video, audio, and text for multimodal emotion understanding. Grounded in the theory of emotional intelligence, it focuses on three core competencies: basic emotion recognition, conversational emotion understanding, and social complex emotion analysis. It features 13 real-world evaluation scenarios, including music emotion recognition, stock review emotion analysis, multi-party conversation emotion recognition, and humor/sarcasm detection. By simulating the complex and dynamic emotional expressions found in human-human and human-machine interactions, it fills the gap left by existing single-modality or static emotion recognition datasets.
![Alt text](images/intro_1.png)

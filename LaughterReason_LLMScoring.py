"""
==========================================================
EmoBench Laughter Reason Evaluator
==========================================================

This script automatically evaluates model-generated reasoning
for *why a video audience laughed*, compared with a reference.

It uses an OpenAI-compatible API to generate two scores:
1. Logical Judgment Dimension (0â€“5)
2. Multimodal Content Association Dimension (0â€“5)

Each input CSV file must contain:
    video,prediction,expected

Example Input (CSV):
----------------------------------------------------------
video,prediction,expected
vid_001,"The crowd laughed because the man slipped on a banana peel.","Audience laughed when the comedian pretended to slip while exaggerating the fall."
vid_002,"They laughed at a witty pun about time.","The joke was a visual gag: a clock costume falls apart when asked for the time."
----------------------------------------------------------

Example Output (CSV):
----------------------------------------------------------
video,Logical Judgment Dimension,Multimodal Content Association Dimension,Total Score
vid_001,4,4,8
vid_002,2,1,3
Average,3.0,2.5,5.5
----------------------------------------------------------

Usage:
----------------------------------------------------------
python emobench_laughter_reason_evaluator.py \
  --input_dir /path/to/input_csvs \
  --output_dir /path/to/output_csvs \
  --model "Qwen/Qwen2.5-72B-Instruct" \
  --base_url "https://api.siliconflow.cn/v1" \
  --api_key_env "SILICONFLOW_API_KEY"
----------------------------------------------------------

Notes:
- Set your API key in an environment variable (e.g., `export SILICONFLOW_API_KEY="your_key"`).
- Each row is evaluated with two API calls (one for each dimension).
- The script appends average scores to the end of each result file.

Author: Open Version by ChatGPT (GPT-5)
License: MIT
==========================================================
"""

import os
import re
import csv
import argparse
import pandas as pd
from tqdm import tqdm
from openai import OpenAI


# =======================
# Prompt Builders
# =======================

def create_prompt_logical(reference_reason, generated_reason):
    prompt = (
        "You need to evaluate the quality of a model-generated reasoning for why a video audience laughed. "
        "You will be provided with two reasons for laughter reasoning:\n"
        "1. The reason for laughter generated by the model.\n"
        "2. The reference reason for laughter annotated manually (as a benchmark).\n\n"
        "Please score based on the following dimension, with a maximum of 5 points:\n\n"
        "**Logical Judgment Dimension:** Based on the reference reason, evaluate the model-generated reason in terms of logical clarity, "
        "the rationality of the causal chain, and coherence with the context.\n\n"
        "**Scoring Criteria:**\n"
        "1. **1 Point:** The reasoning lacks logic, with unclear or missing causal relationships, and is incoherent with the context.\n"
        "2. **2 Points:** The reasoning has some logical flaws and partial causal connections but is largely incoherent with the context.\n"
        "3. **3 Points:** The reasoning is moderately logical, with clear causal links, though some minor inconsistencies with the context exist.\n"
        "4. **4 Points:** The reasoning is mostly logical, with well-defined causal relationships and strong coherence with the context.\n"
        "5. **5 Points:** The reasoning is fully logical, with clear and rational causal chains and excellent coherence with the context.\n\n"
        "Input:\n"
        f"Reference Reason: {reference_reason}\n"
        f"Generated Reason: {generated_reason}\n\n"
        "Output Format:\n"
        "Logical Judgment Dimension: <score>"
    )
    return prompt


def create_prompt_multimodal(reference_reason, generated_reason):
    prompt = (
        "You need to evaluate the quality of a model-generated reasoning for why a video audience laughed. "
        "You will be provided with two reasons for laughter reasoning:\n"
        "1. The reason for laughter generated by the model.\n"
        "2. The reference reason for laughter annotated manually (as a benchmark).\n\n"
        "Please score based on the following dimension, with a maximum of 5 points:\n\n"
        "**Multimodal Content Association Dimension:** Based on the reference reason, evaluate whether the generated text accurately reflects "
        "the interactions between language, visual, audio, and other modal contents, especially whether these contents are consistent with the triggers for laughter.\n\n"
        "**Scoring Criteria:**\n"
        "1. **1 Point:** The reasoning fails to associate with multimodal content, showing no consistency with language, visual, audio, or other modalities.\n"
        "2. **2 Points:** The reasoning shows minimal association with multimodal content, with limited consistency and several mismatches.\n"
        "3. **3 Points:** The reasoning moderately reflects multimodal interactions, maintaining some consistency but with noticeable gaps.\n"
        "4. **4 Points:** The reasoning strongly associates with multimodal content, showing clear consistency with most language, visual, audio, and other modalities.\n"
        "5. **5 Points:** The reasoning perfectly captures and reflects the interactions between all relevant multimodal contents, with complete consistency with the triggers for laughter.\n\n"
        "Input:\n"
        f"Reference Reason: {reference_reason}\n"
        f"Generated Reason: {generated_reason}\n\n"
        "Output Format:\n"
        "Multimodal Content Association Dimension: <score>"
    )
    return prompt


# =======================
# Score Extraction
# =======================

def extract_logical_score(response: str):
    match = re.search(r"Logical Judgment Dimension:\s*(\d+)", response or "", re.IGNORECASE)
    return int(match.group(1)) if match else None

def extract_multimodal_score(response: str):
    match = re.search(r"Multimodal Content Association Dimension:\s*(\d+)", response or "", re.IGNORECASE)
    return int(match.group(1)) if match else None


# =======================
# API Client Initialization
# =======================

def init_client(api_key_env: str, base_url: str):
    api_key = os.getenv(api_key_env)
    if not api_key:
        raise RuntimeError(f"Missing API key. Please set environment variable {api_key_env}.")
    return OpenAI(api_key=api_key, base_url=base_url if base_url else None)


def generate_response(client: OpenAI, model: str, prompt: str) -> str:
    try:
        resp = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
        )
        return (resp.choices[0].message.content or "").strip()
    except Exception as e:
        tqdm.write(f"[API Error] {e}")
        return ""


# =======================
# Evaluation Logic
# =======================

def process_combined_evaluation(client: OpenAI, model: str, input_file_path: str, output_file_path: str):
    try:
        df = pd.read_csv(input_file_path)
    except Exception as e:
        print(f"Error reading {input_file_path}: {e}")
        return

    required_columns = {'video', 'prediction', 'expected'}
    if not required_columns.issubset(df.columns):
        print(f"Missing required columns in {input_file_path}. Required: {sorted(required_columns)}")
        return

    os.makedirs(os.path.dirname(output_file_path), exist_ok=True)
    total_logical = total_multimodal = count = 0

    with open(output_file_path, mode='w', newline='', encoding='utf-8') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(['video', 'Logical Judgment Dimension', 'Multimodal Content Association Dimension', 'Total Score'])

        for i, (_, row) in enumerate(tqdm(df.iterrows(), total=df.shape[0], desc=f"Processing {os.path.basename(input_file_path)}"), start=1):
            video = str(row['video'])
            generated_reason = str(row['prediction'])
            reference_reason = str(row['expected'])

            prompt_logical = create_prompt_logical(reference_reason, generated_reason)
            prompt_multimodal = create_prompt_multimodal(reference_reason, generated_reason)

            response_logical = generate_response(client, model, prompt_logical)
            response_multimodal = generate_response(client, model, prompt_multimodal)

            logical_score = extract_logical_score(response_logical) or 0
            multimodal_score = extract_multimodal_score(response_multimodal) or 0

            total_score = logical_score + multimodal_score
            total_logical += logical_score
            total_multimodal += multimodal_score
            count += 1

            writer.writerow([video, logical_score, multimodal_score, total_score])
            tqdm.write(f"[{i}/{df.shape[0]}] video={video} | Logical={logical_score} | Multimodal={multimodal_score} | Total={total_score}")

    avg_logical = (total_logical / count) if count else 0
    avg_multimodal = (total_multimodal / count) if count else 0
    avg_total = ((total_logical + total_multimodal) / count) if count else 0

    with open(output_file_path, mode='a', newline='', encoding='utf-8') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(['Average', round(avg_logical, 2), round(avg_multimodal, 2), round(avg_total, 2)])

    print(f"âœ… Finished: {input_file_path} -> {output_file_path}")
    print(f"Average Scores â€” Logical: {avg_logical:.2f}, Multimodal: {avg_multimodal:.2f}, Total: {avg_total:.2f}")


# =======================
# Main
# =======================

def main():
    parser = argparse.ArgumentParser(description="Evaluate model-generated laughter reasons on Logical and Multimodal dimensions.")
    parser.add_argument("--input_dir", required=True, help="Folder containing input CSV files.")
    parser.add_argument("--output_dir", required=True, help="Folder to save output CSV files.")
    parser.add_argument("--model", default="Qwen/Qwen2.5-72B-Instruct", help="Model name on the provider.")
    parser.add_argument("--base_url", default="", help="OpenAI-compatible API base URL (e.g., https://api.siliconflow.cn/v1).")
    parser.add_argument("--api_key_env", default="OPENAI_API_KEY", help="Environment variable storing the API key.")
    args = parser.parse_args()

    if not os.path.isdir(args.input_dir):
        print(f"Error: Input folder {args.input_dir} does not exist.")
        return

    client = init_client(api_key_env=args.api_key_env, base_url=args.base_url)

    csv_files = [f for f in os.listdir(args.input_dir) if f.lower().endswith('.csv')]
    if not csv_files:
        print(f"No .csv files found in {args.input_dir}.")
        return

    print(f"Found {len(csv_files)} CSV file(s). Starting processing...")

    for csv_file in csv_files:
        input_file_path = os.path.join(args.input_dir, csv_file)
        output_file_path = os.path.join(args.output_dir, f"combined_{csv_file}")
        process_combined_evaluation(client, args.model, input_file_path, output_file_path)

    print("ðŸŽ¯ All files processed.")


if __name__ == "__main__":
    main()
